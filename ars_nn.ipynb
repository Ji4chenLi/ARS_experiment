{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer():\n",
    "    '''\n",
    "    A class to record and normalize the \n",
    "    state encountered and calculate the \n",
    "    mean and covariance. \n",
    "    '''\n",
    "    def __init__(self, nb_inputs, mode):\n",
    "        '''\n",
    "        mode=0 for V1/V1-t\n",
    "        mode=1 for V2/V2-t\n",
    "        '''\n",
    "        self.n = np.zeros(nb_inputs)\n",
    "        self.mean = np.zeros(nb_inputs)\n",
    "        self.mean_diff = np.zeros(nb_inputs)\n",
    "        self.var = np.zeros(nb_inputs)\n",
    "        self.mode = mode\n",
    "    \n",
    "    def observe(self, x):\n",
    "        '''\n",
    "        Update the statistics\n",
    "        '''\n",
    "        self.n += 1.\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
    "    \n",
    "    def normalize(self, ob):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        if self.mode == 0:\n",
    "            return ob\n",
    "        elif self.mode == 1:\n",
    "            return (ob - obs_mean) / obs_std\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(object):\n",
    "\n",
    "    def __init__(self,env_seed=123,\n",
    "                 env_name='Swimmer-v2',\n",
    "                 policy_params=None,\n",
    "                 layer_size=None,\n",
    "                 delta_std=0.02,\n",
    "                 ):\n",
    "        \n",
    "        # Initialize OpenAI environment for each worker\n",
    "        \n",
    "        self.env = gym.make(env_name)\n",
    "        self.env.seed(env_seed)\n",
    "        self.delta_std = delta_std\n",
    "        \n",
    "    def explore(self, env, normalizer, policy, direction = None, delta = None):\n",
    "        '''\n",
    "        Based on the input policy, explore the environment\n",
    "        '''\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        num_plays = 0.\n",
    "        sum_rewards = 0\n",
    "        for num_plays in itertools.count():\n",
    "            normalizer.observe(state)\n",
    "            state = normalizer.normalize(state)\n",
    "            action = policy.evaluate(state, delta, self.delta_std, direction)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            sum_rewards += reward\n",
    "            if done or num_plays > 1000:\n",
    "                break\n",
    "        return sum_rewards\n",
    "\n",
    "    def do_rollout(self, num_rollouts, normalizer, policy): \n",
    "        '''\n",
    "        Generate random direction, and then evaluate the\n",
    "        policy positively and negatively\n",
    "        '''\n",
    "        # Initializing the perturbations deltas and the positive/negative rewards\n",
    "        deltas = policy.sample_deltas(num_rollouts)\n",
    "        positive_rewards = [0] * num_rollouts\n",
    "        negative_rewards = [0] * num_rollouts\n",
    "\n",
    "        # Getting the positive rewards in the positive directions\n",
    "        for k in range(num_rollouts):\n",
    "            positive_rewards[k] = self.explore(env, normalizer, policy, direction = \"positive\", delta = deltas[k])\n",
    "        # Getting the negative rewards in the negative/opposite directions\n",
    "        for k in range(num_rollouts):\n",
    "            negative_rewards[k] = self.explore(env, normalizer, policy, direction = \"negative\", delta = deltas[k])\n",
    "\n",
    "        return {'deltas': deltas, 'positive_rewards': positive_rewards, 'negative_rewards': negative_rewards}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARSLearner(object):\n",
    "    \"\"\" \n",
    "    Object class implementing the ARS algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 env_name='Swimmer-v2',\n",
    "                 policy_params=None,\n",
    "                 learning_rate=0.02,\n",
    "                 delta_std=0.02, \n",
    "                 num_iter=1000,\n",
    "                 layer_size=None,\n",
    "                 seed=123,\n",
    "                 mode=0\n",
    "                ):\n",
    "        print('Here')\n",
    "        self.policy = Policy(policy_params['ob_dim'], policy_params['ac_dim'],layer_size)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iter = num_iter\n",
    "        self.normalizer = Normalizer(policy_params['ob_dim'],mode)          \n",
    "        self.worker = Worker(env_seed=seed + 7,\n",
    "                             env_name=env_name,\n",
    "                             policy_params=policy_params,\n",
    "                             layer_size=layer_size,\n",
    "                             delta_std=delta_std)\n",
    "        \n",
    "    def train(self,num_rollouts,max_b):\n",
    "\n",
    "        for t in range(self.num_iter):\n",
    "            t1 = time.time() \n",
    "            result_dict = self.worker.do_rollout(num_rollouts, self.normalizer, self.policy) \n",
    "            #Gather the result\n",
    "\n",
    "            deltas = result_dict['deltas']\n",
    "            positive_rewards = result_dict['positive_rewards']\n",
    "            negative_rewards = result_dict['negative_rewards']\n",
    "\n",
    "            #Update the weight\n",
    "            all_rewards = np.array(positive_rewards + negative_rewards)\n",
    "            sigma_r = all_rewards.std()\n",
    "\n",
    "            # Sorting the rollouts by the max(r_pos, r_neg) and selecting the best directions\n",
    "            scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "            order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:max_b]\n",
    "            rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
    "\n",
    "            # Updating our policy\n",
    "            self.policy.update(rollouts, sigma_r, self.learning_rate, max_b)\n",
    "\n",
    "            # Printing the final reward of the policy after the update\n",
    "            reward_evaluation = self.worker.explore(env, self.normalizer, self.policy)\n",
    "            t2 = time.time() \n",
    "            print('total time of one step:', t2 - t1,', reward=',reward_evaluation,', iter ', t,' done')      \n",
    "    \n",
    "#     def evaluate(self,test_num,isRender):\n",
    "#         returns = []\n",
    "#         for i in range(test_num):\n",
    "#             print('iter', i)\n",
    "#             state = env.reset()\n",
    "#             done = False\n",
    "#             num_plays = 0\n",
    "#             sum_rewards = 0\n",
    "#             while not done and num_plays < 1000:\n",
    "#                 state = self.normalizer.normalize(state)\n",
    "#                 action = self.policy.evaluate(state, None, delta_std, None)\n",
    "#                 state, reward, done, _ = env.step(action)\n",
    "#                 sum_rewards += reward\n",
    "#                 num_plays += 1\n",
    "#                 if isRender:\n",
    "#                     env.render()   \n",
    "#                 if num_plays % 100 == 0: print(\"%i/%i\"%(num_plays, env.spec.timestep_limit))\n",
    "\n",
    "#             returns.append(totalr)\n",
    "\n",
    "#         print('returns', returns)\n",
    "#         print('mean return', np.mean(returns))\n",
    "#         print('std of return', np.std(returns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the policy on one specific direction and over one episode\n",
    "class Policy():\n",
    "    \"\"\"\n",
    "    Policy Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, output_size, layer_size):\n",
    "        self.depth = len(layer_size)\n",
    "        self.weights = []\n",
    "        for i_layer in layer_size:\n",
    "            self.weights.append(np.random.rand(*i_layer))\n",
    "    \n",
    "    def evaluate(self, input, delta = None, delta_std =  0.02, direction = None):\n",
    "        layer = input\n",
    "        if direction is None: \n",
    "            for i in range(self.depth - 1):\n",
    "                layer = relu(self.weights[i].dot(layer))\n",
    "            return np.tanh(self.weights[self.depth - 1].dot(layer)) * 2\n",
    "        elif direction == \"positive\":\n",
    "            for i in range(self.depth - 1):\n",
    "                layer = relu((self.weights[i] + delta_std * delta[i]).dot(layer))\n",
    "            return np.tanh((self.weights[self.depth - 1] + delta_std * delta[self.depth - 1]).dot(layer)) * 2\n",
    "        else:\n",
    "            for i in range(self.depth - 1):\n",
    "                layer = relu((self.weights[i] - delta_std * delta[i]).dot(layer))\n",
    "            return np.tanh((self.weights[self.depth - 1] - delta_std * delta[self.depth - 1]).dot(layer)) * 2\n",
    "                                                      \n",
    "    def sample_deltas(self,num_rollouts):\n",
    "        delta = []\n",
    "        for _ in range(num_rollouts):\n",
    "            d = []\n",
    "            for i in range(self.depth):\n",
    "                d.append(np.random.randn(*self.weights[i].shape))\n",
    "            delta.append(d)\n",
    "        return delta\n",
    "    \n",
    "    def update(self, rollouts, sigma_r,learning_rate,max_b):\n",
    "        step = []\n",
    "        for i in range(self.depth):\n",
    "            step.append(np.zeros(self.weights[i].shape))\n",
    "            \n",
    "        for r_pos, r_neg, delta in rollouts:\n",
    "            for i in range(self.depth):\n",
    "                d = delta[i]\n",
    "                step[i] += (r_pos - r_neg) * d\n",
    "                \n",
    "        for i in range(self.depth):\n",
    "            self.weights[i] += learning_rate / (max_b * sigma_r) * step[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(input):\n",
    "    return input * (input > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ob_dim': 3, 'ac_dim': 1}\n",
      "Here\n",
      "total time of one step: 0.23935484886169434 , reward= -1306.839634383308 , iter  0  done\n",
      "total time of one step: 0.24234342575073242 , reward= -1374.7820031348629 , iter  1  done\n",
      "total time of one step: 0.2423412799835205 , reward= -1491.29455497441 , iter  2  done\n",
      "total time of one step: 0.23978924751281738 , reward= -1657.3021002586788 , iter  3  done\n",
      "total time of one step: 0.25702452659606934 , reward= -1437.3680548623704 , iter  4  done\n",
      "total time of one step: 0.259885311126709 , reward= -1559.6627039368725 , iter  5  done\n",
      "total time of one step: 0.24335265159606934 , reward= -1390.6419605357537 , iter  6  done\n",
      "total time of one step: 0.25481700897216797 , reward= -1710.4890150623992 , iter  7  done\n",
      "total time of one step: 0.2677340507507324 , reward= -1471.5218067611936 , iter  8  done\n",
      "total time of one step: 0.25530052185058594 , reward= -1634.8068578714192 , iter  9  done\n",
      "total time of one step: 0.25711631774902344 , reward= -1489.9826123976568 , iter  10  done\n",
      "total time of one step: 0.23873639106750488 , reward= -1575.0794196247816 , iter  11  done\n",
      "total time of one step: 0.24696946144104004 , reward= -1578.6476837015346 , iter  12  done\n",
      "total time of one step: 0.265547513961792 , reward= -1633.260560485735 , iter  13  done\n",
      "total time of one step: 0.25309062004089355 , reward= -1555.6248127219048 , iter  14  done\n",
      "total time of one step: 0.2933156490325928 , reward= -1621.0045643026756 , iter  15  done\n",
      "total time of one step: 0.27725815773010254 , reward= -1385.5667592122024 , iter  16  done\n",
      "total time of one step: 0.26215457916259766 , reward= -1353.8172292407285 , iter  17  done\n",
      "total time of one step: 0.25240397453308105 , reward= -1745.5346186689312 , iter  18  done\n",
      "total time of one step: 0.2573108673095703 , reward= -1640.7765710295134 , iter  19  done\n",
      "total time of one step: 0.24819493293762207 , reward= -1673.860227005534 , iter  20  done\n",
      "total time of one step: 0.24883198738098145 , reward= -1515.7652273983142 , iter  21  done\n",
      "total time of one step: 0.24994945526123047 , reward= -1454.5409437302233 , iter  22  done\n",
      "total time of one step: 0.238389253616333 , reward= -1728.309167110887 , iter  23  done\n",
      "total time of one step: 0.24694228172302246 , reward= -1632.913013249253 , iter  24  done\n",
      "total time of one step: 0.24409818649291992 , reward= -1605.0517804934595 , iter  25  done\n",
      "total time of one step: 0.25249218940734863 , reward= -1641.7223657123036 , iter  26  done\n",
      "total time of one step: 0.2413487434387207 , reward= -1511.0251470410992 , iter  27  done\n",
      "total time of one step: 0.24500346183776855 , reward= -1602.8189630918164 , iter  28  done\n",
      "total time of one step: 0.2680034637451172 , reward= -1597.1027855981633 , iter  29  done\n",
      "total time of one step: 0.26912975311279297 , reward= -1572.7859644959433 , iter  30  done\n",
      "total time of one step: 0.2642858028411865 , reward= -1587.486751814363 , iter  31  done\n",
      "total time of one step: 0.2598083019256592 , reward= -1353.8389737777213 , iter  32  done\n",
      "total time of one step: 0.26542019844055176 , reward= -1579.37500565376 , iter  33  done\n",
      "total time of one step: 0.25202488899230957 , reward= -1455.7338061740818 , iter  34  done\n",
      "total time of one step: 0.27087831497192383 , reward= -1616.458043621273 , iter  35  done\n",
      "total time of one step: 0.25811100006103516 , reward= -1474.0955180143483 , iter  36  done\n",
      "total time of one step: 0.261216402053833 , reward= -1623.812177154245 , iter  37  done\n",
      "total time of one step: 0.24231910705566406 , reward= -1594.6979578769349 , iter  38  done\n",
      "total time of one step: 0.2529480457305908 , reward= -1774.015164141507 , iter  39  done\n",
      "total time of one step: 0.2618746757507324 , reward= -1616.0847060012993 , iter  40  done\n",
      "total time of one step: 0.2526552677154541 , reward= -1566.0599484333138 , iter  41  done\n",
      "total time of one step: 0.2641725540161133 , reward= -1600.120484361964 , iter  42  done\n",
      "total time of one step: 0.2630801200866699 , reward= -1783.148813722236 , iter  43  done\n",
      "total time of one step: 0.276897668838501 , reward= -1496.3268112020833 , iter  44  done\n",
      "total time of one step: 0.2563929557800293 , reward= -1508.2231859881072 , iter  45  done\n",
      "total time of one step: 0.252394437789917 , reward= -1515.5462984539847 , iter  46  done\n",
      "total time of one step: 0.2550358772277832 , reward= -1496.045273801256 , iter  47  done\n",
      "total time of one step: 0.26502490043640137 , reward= -1477.4244730438043 , iter  48  done\n",
      "total time of one step: 0.2509920597076416 , reward= -1586.3178580318136 , iter  49  done\n",
      "total time of one step: 0.26131510734558105 , reward= -1748.1921574384385 , iter  50  done\n",
      "total time of one step: 0.24672651290893555 , reward= -1725.3492955387094 , iter  51  done\n",
      "total time of one step: 0.26204562187194824 , reward= -1522.0495937425258 , iter  52  done\n",
      "total time of one step: 0.25803279876708984 , reward= -1536.1092148909902 , iter  53  done\n",
      "total time of one step: 0.2939798831939697 , reward= -1385.739005230871 , iter  54  done\n",
      "total time of one step: 0.2631955146789551 , reward= -1639.5773217977135 , iter  55  done\n",
      "total time of one step: 0.2731297016143799 , reward= -1626.0576746686888 , iter  56  done\n",
      "total time of one step: 0.27574992179870605 , reward= -1608.1283711101057 , iter  57  done\n",
      "total time of one step: 0.26226043701171875 , reward= -1609.8676940248133 , iter  58  done\n",
      "total time of one step: 0.2533886432647705 , reward= -1528.8793009848862 , iter  59  done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-bd80fb06d18e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m                      mode)\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mLearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-3deed2de80cd>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, num_rollouts, max_b)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mresult_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_rollout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[1;31m#Gather the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-b323a2d3def9>\u001b[0m in \u001b[0;36mdo_rollout\u001b[1;34m(self, num_rollouts, normalizer, policy)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# Getting the negative rewards in the negative/opposite directions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mnegative_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"negative\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'deltas'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'positive_rewards'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpositive_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'negative_rewards'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnegative_rewards\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-b323a2d3def9>\u001b[0m in \u001b[0;36mexplore\u001b[1;34m(self, env, normalizer, policy, direction, delta)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mnormalizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0msum_rewards\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-0ec9a8377f09>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, input, delta, delta_std, direction)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdelta_std\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdelta_std\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msample_deltas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Test the algorithm\n",
    "env_name = 'Pendulum-v0'\n",
    "env = gym.make(env_name)\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.shape[0]\n",
    "policy_params={'ob_dim':ob_dim,'ac_dim':ac_dim}\n",
    "print(policy_params)\n",
    "learning_rate=0.005\n",
    "delta_std=0.01\n",
    "num_iter=10000\n",
    "num_rollouts = 16\n",
    "max_b=4\n",
    "mid_size = 20\n",
    "layer_size = [[mid_size, ob_dim], [ac_dim, mid_size]]\n",
    "seed = 123\n",
    "mode=1\n",
    "\n",
    "Learner = ARSLearner(env_name,\n",
    "                     policy_params,\n",
    "                     learning_rate,\n",
    "                     delta_std, \n",
    "                     num_iter,\n",
    "                     layer_size,\n",
    "                     seed,\n",
    "                     mode)\n",
    "\n",
    "Learner.train(num_rollouts,max_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
