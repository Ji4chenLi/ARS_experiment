{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer():\n",
    "    '''\n",
    "    A class to record and normalize the \n",
    "    state encountered and calculate the \n",
    "    mean and covariance. \n",
    "    '''\n",
    "    def __init__(self, nb_inputs, mode):\n",
    "        '''\n",
    "        mode=0 for V1/V1-t\n",
    "        mode=1 for V2/V2-t\n",
    "        '''\n",
    "        self.n = np.zeros(nb_inputs)\n",
    "        self.mean = np.zeros(nb_inputs)\n",
    "        self.mean_diff = np.zeros(nb_inputs)\n",
    "        self.var = np.zeros(nb_inputs)\n",
    "        self.mode = mode\n",
    "    \n",
    "    def observe(self, x):\n",
    "        '''\n",
    "        Update the statistics\n",
    "        '''\n",
    "        self.n += 1.\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
    "    \n",
    "    def normalize(self, ob):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        if self.mode == 0:\n",
    "            return ob\n",
    "        elif self.mode == 1:\n",
    "            return (ob - obs_mean) / obs_std\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(object):\n",
    "\n",
    "    def __init__(self,env_seed=123,\n",
    "                 env_name='Swimmer-v2',\n",
    "                 policy_params=None,\n",
    "                 delta_std=0.02,\n",
    "                 ):\n",
    "        \n",
    "        # Initialize OpenAI environment for each worker\n",
    "        \n",
    "        self.env = gym.make(env_name)\n",
    "        self.env.seed(env_seed)\n",
    "        self.delta_std = delta_std\n",
    "        \n",
    "    def explore(self, env, normalizer, policy, direction = None, delta = None):\n",
    "        '''\n",
    "        Based on the input policy, explore the environment\n",
    "        '''\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        num_plays = 0.\n",
    "        sum_rewards = 0\n",
    "        for num_plays in itertools.count():\n",
    "            normalizer.observe(state)\n",
    "            state = normalizer.normalize(state)\n",
    "            action = policy.evaluate(state, delta, self.delta_std, direction)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            sum_rewards += reward\n",
    "            if done or num_plays > 1000:\n",
    "                break\n",
    "        return sum_rewards\n",
    "\n",
    "    def do_rollout(self, num_rollouts, normalizer, policy): \n",
    "        '''\n",
    "        Generate random direction, and then evaluate the\n",
    "        policy positively and negatively\n",
    "        '''\n",
    "        # Initializing the perturbations deltas and the positive/negative rewards\n",
    "        deltas = policy.sample_deltas(num_rollouts)\n",
    "        positive_rewards = [0] * num_rollouts\n",
    "        negative_rewards = [0] * num_rollouts\n",
    "\n",
    "        # Getting the positive rewards in the positive directions\n",
    "        for k in range(num_rollouts):\n",
    "            positive_rewards[k] = self.explore(env, normalizer, policy, direction = \"positive\", delta = deltas[k])\n",
    "        # Getting the negative rewards in the negative/opposite directions\n",
    "        for k in range(num_rollouts):\n",
    "            negative_rewards[k] = self.explore(env, normalizer, policy, direction = \"negative\", delta = deltas[k])\n",
    "\n",
    "        return {'deltas': deltas, 'positive_rewards': positive_rewards, 'negative_rewards': negative_rewards}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARSLearner(object):\n",
    "    \"\"\" \n",
    "    Object class implementing the ARS algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 env_name='Swimmer-v2',\n",
    "                 policy_params=None,\n",
    "                 learning_rate=0.02,\n",
    "                 delta_std=0.02, \n",
    "                 num_iter=1000,\n",
    "                 seed=123,\n",
    "                 mode=0\n",
    "                ):\n",
    "        self.policy = Policy(policy_params['ob_dim'], policy_params['ac_dim'])\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iter = num_iter\n",
    "        self.normalizer = Normalizer(policy_params['ob_dim'],mode)          \n",
    "        self.worker = Worker(env_seed=seed + 7,\n",
    "                             env_name=env_name,\n",
    "                             policy_params=policy_params,\n",
    "                             delta_std=delta_std)\n",
    "        \n",
    "    def train(self,num_rollouts,max_b):\n",
    "\n",
    "        for t in range(self.num_iter):\n",
    "            t1 = time.time() \n",
    "            result_dict = self.worker.do_rollout(num_rollouts, self.normalizer, self.policy) \n",
    "            #Gather the result\n",
    "\n",
    "            deltas = result_dict['deltas']\n",
    "            positive_rewards = result_dict['positive_rewards']\n",
    "            negative_rewards = result_dict['negative_rewards']\n",
    "\n",
    "            #Update the weight\n",
    "            all_rewards = np.array(positive_rewards + negative_rewards)\n",
    "            sigma_r = all_rewards.std()\n",
    "\n",
    "            # Sorting the rollouts by the max(r_pos, r_neg) and selecting the best directions\n",
    "            scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "            order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:max_b]\n",
    "            rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
    "\n",
    "            # Updating our policy\n",
    "            self.policy.update(rollouts, sigma_r, self.learning_rate, max_b)\n",
    "\n",
    "            # Printing the final reward of the policy after the update\n",
    "            reward_evaluation = self.worker.explore(env, self.normalizer, self.policy)\n",
    "            t2 = time.time() \n",
    "            print('total time of one step:', t2 - t1,', reward=' ,reward_evaluation,', iter ', t,' done')      \n",
    "    \n",
    "    def evaluate(self,test_num,isRender):\n",
    "        returns = []\n",
    "        print(self.policy.theta)\n",
    "        for i in range(test_num):\n",
    "            print('iter', i)\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            num_plays = 0\n",
    "            sum_rewards = 0\n",
    "            while not done and num_plays < 1000:\n",
    "                state = self.normalizer.normalize(state)\n",
    "                action = self.policy.evaluate(state, None, delta_std, None)\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                sum_rewards += reward\n",
    "                num_plays += 1\n",
    "                if isRender:\n",
    "                    env.render()   \n",
    "                if num_plays % 100 == 0: print(\"%i/%i\"%(steps, env.spec.timestep_limit))\n",
    "\n",
    "            returns.append(totalr)\n",
    "\n",
    "        print('returns', returns)\n",
    "        print('mean return', np.mean(returns))\n",
    "        print('std of return', np.std(returns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Building the AI\n",
    "\n",
    "class Policy():\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.theta = np.zeros((output_size, input_size))\n",
    "    \n",
    "    def evaluate(self, input, delta = None, delta_std =  0.02, direction = None):\n",
    "        if direction is None:\n",
    "            return self.theta.dot(input)\n",
    "        elif direction == \"positive\":\n",
    "            return (self.theta + delta_std*delta).dot(input)\n",
    "        else:\n",
    "            return (self.theta - delta_std*delta).dot(input)\n",
    "    \n",
    "    def sample_deltas(self,num_rollouts):\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(num_rollouts)]\n",
    "    \n",
    "    def update(self, rollouts, sigma_r,learning_rate,max_b):\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for r_pos, r_neg, d in rollouts:\n",
    "            step += (r_pos - r_neg) * d\n",
    "        self.theta += learning_rate / (max_b * sigma_r) * step\n",
    "\n",
    "# Exploring the policy on one specific direction and over one episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time of one step: 2.5376009941101074 , reward= 0.8397265753729617 , iter  0  done\n",
      "total time of one step: 2.4842889308929443 , reward= -0.07918574750612375 , iter  1  done\n",
      "total time of one step: 2.559300184249878 , reward= -0.7451602830053257 , iter  2  done\n",
      "total time of one step: 2.5057170391082764 , reward= 0.6103123485866337 , iter  3  done\n",
      "total time of one step: 2.447580337524414 , reward= -1.4011496769055614 , iter  4  done\n",
      "total time of one step: 3.044841766357422 , reward= -0.4055209406903972 , iter  5  done\n",
      "total time of one step: 3.1943259239196777 , reward= -0.5275213573039131 , iter  6  done\n",
      "total time of one step: 3.960387706756592 , reward= -0.9270337793239882 , iter  7  done\n",
      "total time of one step: 3.232692003250122 , reward= -0.6171118502465192 , iter  8  done\n",
      "total time of one step: 4.106821060180664 , reward= 6.36267476382277 , iter  9  done\n",
      "total time of one step: 4.690614938735962 , reward= 5.753988707362596 , iter  10  done\n",
      "total time of one step: 3.492668867111206 , reward= 12.16591349693253 , iter  11  done\n",
      "total time of one step: 2.5984597206115723 , reward= 11.200839646108362 , iter  12  done\n",
      "total time of one step: 2.6142051219940186 , reward= 714.1116370468396 , iter  13  done\n",
      "total time of one step: 2.883817195892334 , reward= 1024.1036457061662 , iter  14  done\n",
      "total time of one step: 2.4525907039642334 , reward= 1653.5714675022098 , iter  15  done\n",
      "total time of one step: 2.4195969104766846 , reward= 1614.0133651642996 , iter  16  done\n",
      "total time of one step: 2.4375360012054443 , reward= 1913.228350745571 , iter  17  done\n",
      "total time of one step: 2.3749709129333496 , reward= 1478.0523293682684 , iter  18  done\n",
      "total time of one step: 2.4332408905029297 , reward= 1862.5284254139674 , iter  19  done\n",
      "total time of one step: 2.4324281215667725 , reward= 1746.599666135685 , iter  20  done\n",
      "total time of one step: 2.4703569412231445 , reward= 2042.4801420750832 , iter  21  done\n",
      "total time of one step: 2.496537923812866 , reward= 2528.290987173139 , iter  22  done\n",
      "total time of one step: 2.3167800903320312 , reward= 2541.271158550657 , iter  23  done\n",
      "total time of one step: 2.5290510654449463 , reward= 2147.3512450371377 , iter  24  done\n",
      "total time of one step: 2.357603073120117 , reward= 2277.228181410615 , iter  25  done\n",
      "total time of one step: 2.7661070823669434 , reward= 2306.212915525041 , iter  26  done\n",
      "total time of one step: 2.6925408840179443 , reward= 2697.767220634684 , iter  27  done\n",
      "total time of one step: 3.227576971054077 , reward= 2439.099184721534 , iter  28  done\n",
      "total time of one step: 3.01859188079834 , reward= 2431.7501847804297 , iter  29  done\n",
      "total time of one step: 2.9890072345733643 , reward= 2380.5171335865844 , iter  30  done\n",
      "total time of one step: 3.4966230392456055 , reward= 2513.9154250350243 , iter  31  done\n",
      "total time of one step: 2.916033983230591 , reward= 2557.294068676652 , iter  32  done\n",
      "total time of one step: 3.159954309463501 , reward= 2906.3133303123077 , iter  33  done\n",
      "total time of one step: 2.5373449325561523 , reward= 2295.445647338792 , iter  34  done\n",
      "total time of one step: 2.880795955657959 , reward= 2417.8311336762063 , iter  35  done\n",
      "total time of one step: 2.6101279258728027 , reward= 2855.6407110879313 , iter  36  done\n",
      "total time of one step: 2.6010100841522217 , reward= 2689.372550739647 , iter  37  done\n",
      "total time of one step: 2.4947099685668945 , reward= 2652.751912170717 , iter  38  done\n",
      "total time of one step: 2.5433108806610107 , reward= 3092.210255908296 , iter  39  done\n",
      "total time of one step: 2.4558839797973633 , reward= 3065.381466347461 , iter  40  done\n",
      "total time of one step: 2.840808868408203 , reward= 2817.145769544956 , iter  41  done\n",
      "total time of one step: 2.9113409519195557 , reward= 2697.2304151341864 , iter  42  done\n",
      "total time of one step: 2.388162136077881 , reward= 3168.897452604285 , iter  43  done\n",
      "total time of one step: 2.4391539096832275 , reward= 3052.9041904702635 , iter  44  done\n",
      "total time of one step: 2.683819055557251 , reward= 3094.9303746119217 , iter  45  done\n",
      "total time of one step: 2.452000141143799 , reward= 3332.639863316737 , iter  46  done\n",
      "total time of one step: 2.5727360248565674 , reward= 3015.500238106386 , iter  47  done\n",
      "total time of one step: 2.3504879474639893 , reward= 3252.324219640992 , iter  48  done\n",
      "total time of one step: 2.484618902206421 , reward= 3135.4377129241384 , iter  49  done\n",
      "total time of one step: 2.390531063079834 , reward= 3318.750213288593 , iter  50  done\n",
      "total time of one step: 2.474915027618408 , reward= 3148.1639692505732 , iter  51  done\n",
      "total time of one step: 2.539565086364746 , reward= 3142.335132576868 , iter  52  done\n",
      "total time of one step: 2.356755256652832 , reward= 2856.0743755679828 , iter  53  done\n",
      "total time of one step: 2.4902470111846924 , reward= 3073.7848697765353 , iter  54  done\n",
      "total time of one step: 2.6045031547546387 , reward= 2923.6344629643054 , iter  55  done\n",
      "total time of one step: 2.66899037361145 , reward= 2926.031761067374 , iter  56  done\n",
      "total time of one step: 2.649735927581787 , reward= 2838.5635484423483 , iter  57  done\n",
      "total time of one step: 2.499483823776245 , reward= 2974.586697782802 , iter  58  done\n",
      "total time of one step: 2.5509092807769775 , reward= 3065.036659033549 , iter  59  done\n",
      "total time of one step: 2.5394909381866455 , reward= 2795.508191552844 , iter  60  done\n",
      "total time of one step: 2.329082727432251 , reward= 3015.364166724295 , iter  61  done\n",
      "total time of one step: 2.4258158206939697 , reward= 2857.127262451907 , iter  62  done\n",
      "total time of one step: 2.726707935333252 , reward= 3171.001440361674 , iter  63  done\n",
      "total time of one step: 2.3540828227996826 , reward= 2879.017471458812 , iter  64  done\n",
      "total time of one step: 2.471256971359253 , reward= 2994.7184469459544 , iter  65  done\n",
      "total time of one step: 2.43670392036438 , reward= 2978.025114680924 , iter  66  done\n",
      "total time of one step: 2.460638999938965 , reward= 3337.944792581567 , iter  67  done\n",
      "total time of one step: 2.533874034881592 , reward= 3171.2168328068406 , iter  68  done\n",
      "total time of one step: 2.3974971771240234 , reward= 3014.134597392593 , iter  69  done\n",
      "total time of one step: 2.506685972213745 , reward= 2958.5558551454696 , iter  70  done\n",
      "total time of one step: 2.6752607822418213 , reward= 3047.5373538536764 , iter  71  done\n",
      "total time of one step: 2.4476888179779053 , reward= 3074.0108273848464 , iter  72  done\n",
      "total time of one step: 2.4436490535736084 , reward= 3324.4501855709896 , iter  73  done\n",
      "total time of one step: 2.373136043548584 , reward= 3075.5433302880197 , iter  74  done\n",
      "total time of one step: 2.4437901973724365 , reward= 2880.330088833532 , iter  75  done\n",
      "total time of one step: 2.3509912490844727 , reward= 3233.6602929112573 , iter  76  done\n",
      "total time of one step: 2.6306869983673096 , reward= 3060.1593541408815 , iter  77  done\n",
      "total time of one step: 2.5808181762695312 , reward= 3033.0638785821593 , iter  78  done\n",
      "total time of one step: 2.423452138900757 , reward= 3201.528143554988 , iter  79  done\n",
      "total time of one step: 2.531538963317871 , reward= 3116.2399965625127 , iter  80  done\n",
      "total time of one step: 2.4727888107299805 , reward= 3217.4290094798803 , iter  81  done\n",
      "total time of one step: 2.459578037261963 , reward= 3430.02961421485 , iter  82  done\n",
      "total time of one step: 2.69832706451416 , reward= 3581.7279580241698 , iter  83  done\n",
      "total time of one step: 2.572289228439331 , reward= 3501.6763938726326 , iter  84  done\n",
      "total time of one step: 2.700601100921631 , reward= 3499.6228036404086 , iter  85  done\n",
      "total time of one step: 2.735274076461792 , reward= 3538.041761133082 , iter  86  done\n",
      "total time of one step: 2.4192728996276855 , reward= 3696.3835326738335 , iter  87  done\n",
      "total time of one step: 2.4970576763153076 , reward= 3498.83289276629 , iter  88  done\n",
      "total time of one step: 2.69930100440979 , reward= 3418.624408008824 , iter  89  done\n",
      "total time of one step: 2.8319411277770996 , reward= 3280.115337954329 , iter  90  done\n",
      "total time of one step: 3.001436948776245 , reward= 3693.6389374718287 , iter  91  done\n",
      "total time of one step: 2.773514986038208 , reward= 3615.600282155524 , iter  92  done\n",
      "total time of one step: 3.061188220977783 , reward= 3571.159398035864 , iter  93  done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time of one step: 2.5127439498901367 , reward= 3599.00023899927 , iter  94  done\n",
      "total time of one step: 2.4081668853759766 , reward= 3752.789980804012 , iter  95  done\n",
      "total time of one step: 2.544081926345825 , reward= 3500.0216232032412 , iter  96  done\n",
      "total time of one step: 2.2697861194610596 , reward= 3958.472986023582 , iter  97  done\n",
      "total time of one step: 2.8822860717773438 , reward= 3627.6032916835447 , iter  98  done\n",
      "total time of one step: 2.6118998527526855 , reward= 3390.7206414766897 , iter  99  done\n",
      "total time of one step: 2.456216812133789 , reward= 3542.6670429539276 , iter  100  done\n",
      "total time of one step: 2.483604907989502 , reward= 3479.0620058484806 , iter  101  done\n",
      "total time of one step: 2.4816651344299316 , reward= 3656.9133347296356 , iter  102  done\n",
      "total time of one step: 2.391184091567993 , reward= 3591.922802735222 , iter  103  done\n",
      "total time of one step: 2.7095460891723633 , reward= 3838.937195439113 , iter  104  done\n",
      "total time of one step: 2.4140689373016357 , reward= 3483.114470481477 , iter  105  done\n",
      "total time of one step: 2.439915895462036 , reward= 3627.639993556151 , iter  106  done\n",
      "total time of one step: 2.6093060970306396 , reward= 3876.2346878373396 , iter  107  done\n",
      "total time of one step: 2.404179334640503 , reward= 3794.823223063055 , iter  108  done\n",
      "total time of one step: 2.3528249263763428 , reward= 3854.1869554715126 , iter  109  done\n",
      "total time of one step: 2.294365882873535 , reward= 3621.879985700953 , iter  110  done\n",
      "total time of one step: 2.4255471229553223 , reward= 3943.796830226601 , iter  111  done\n",
      "total time of one step: 2.382720947265625 , reward= 3937.2308432618956 , iter  112  done\n",
      "total time of one step: 2.2893311977386475 , reward= 3871.7039425356743 , iter  113  done\n",
      "total time of one step: 2.7691869735717773 , reward= 3760.2525079139823 , iter  114  done\n",
      "total time of one step: 2.996027708053589 , reward= 3816.7273378993605 , iter  115  done\n",
      "total time of one step: 3.8878421783447266 , reward= 3576.312255421788 , iter  116  done\n",
      "total time of one step: 2.7857468128204346 , reward= 3639.7744292004454 , iter  117  done\n",
      "total time of one step: 2.8874762058258057 , reward= 3938.5416690088296 , iter  118  done\n",
      "total time of one step: 2.794523000717163 , reward= 3776.265525155338 , iter  119  done\n",
      "total time of one step: 2.587083578109741 , reward= 3593.5052995290275 , iter  120  done\n",
      "total time of one step: 2.7216010093688965 , reward= 3759.7377338820957 , iter  121  done\n",
      "total time of one step: 2.405379056930542 , reward= 3525.208840133506 , iter  122  done\n",
      "total time of one step: 2.445499897003174 , reward= 3438.5392799786714 , iter  123  done\n",
      "total time of one step: 2.5307199954986572 , reward= 3873.107912855727 , iter  124  done\n",
      "total time of one step: 2.600710153579712 , reward= 3770.8665367271788 , iter  125  done\n",
      "total time of one step: 2.5260238647460938 , reward= 3283.3188446802224 , iter  126  done\n",
      "total time of one step: 2.5370571613311768 , reward= 3720.090164815368 , iter  127  done\n",
      "total time of one step: 2.3532049655914307 , reward= 3538.7645965178335 , iter  128  done\n",
      "total time of one step: 2.3684139251708984 , reward= 3399.6932638295452 , iter  129  done\n",
      "total time of one step: 2.4989261627197266 , reward= 3840.4375202377705 , iter  130  done\n",
      "total time of one step: 2.397526979446411 , reward= 3659.4693431932224 , iter  131  done\n",
      "total time of one step: 2.737687110900879 , reward= 3577.7690202053063 , iter  132  done\n",
      "total time of one step: 2.400521993637085 , reward= 3789.5373164640505 , iter  133  done\n",
      "total time of one step: 2.5425281524658203 , reward= 3724.1613122067874 , iter  134  done\n",
      "total time of one step: 2.732564926147461 , reward= 3635.4846009230114 , iter  135  done\n",
      "total time of one step: 2.53059983253479 , reward= 3951.0293217584667 , iter  136  done\n",
      "total time of one step: 2.4440040588378906 , reward= 3909.5152921885574 , iter  137  done\n",
      "total time of one step: 2.692808151245117 , reward= 3814.4054550917936 , iter  138  done\n",
      "total time of one step: 2.541489839553833 , reward= 3362.6707902889116 , iter  139  done\n",
      "total time of one step: 2.5587799549102783 , reward= 3620.049477194394 , iter  140  done\n",
      "total time of one step: 2.50018310546875 , reward= 3562.305590165014 , iter  141  done\n",
      "total time of one step: 2.3016531467437744 , reward= 3815.599990797932 , iter  142  done\n",
      "total time of one step: 2.481757879257202 , reward= 3779.3148061520587 , iter  143  done\n",
      "total time of one step: 2.4058499336242676 , reward= 3767.516272819592 , iter  144  done\n",
      "total time of one step: 2.294584035873413 , reward= 4210.953990068919 , iter  145  done\n",
      "total time of one step: 2.4631149768829346 , reward= 4032.673314628062 , iter  146  done\n",
      "total time of one step: 2.372004747390747 , reward= 4088.3451449041036 , iter  147  done\n",
      "total time of one step: 2.7043910026550293 , reward= 3959.9426383248724 , iter  148  done\n",
      "total time of one step: 2.342698097229004 , reward= 3970.6804137242507 , iter  149  done\n",
      "total time of one step: 2.465080976486206 , reward= 4029.417246070362 , iter  150  done\n",
      "total time of one step: 2.6811370849609375 , reward= 3780.096702116331 , iter  151  done\n",
      "total time of one step: 2.4840738773345947 , reward= 3718.4369011844265 , iter  152  done\n",
      "total time of one step: 2.3691771030426025 , reward= 3945.8650186200803 , iter  153  done\n",
      "total time of one step: 2.372364044189453 , reward= 4066.237220513455 , iter  154  done\n",
      "total time of one step: 2.614470958709717 , reward= 3795.8748968666705 , iter  155  done\n",
      "total time of one step: 2.8526909351348877 , reward= -1008.1010812246311 , iter  156  done\n",
      "total time of one step: 2.59446120262146 , reward= 3948.827305525262 , iter  157  done\n",
      "total time of one step: 2.520310163497925 , reward= 3771.4937038810176 , iter  158  done\n",
      "total time of one step: 2.4385340213775635 , reward= 4212.854266655609 , iter  159  done\n",
      "total time of one step: 2.53924822807312 , reward= 4038.697314417809 , iter  160  done\n",
      "total time of one step: 2.437592029571533 , reward= 3985.547098913394 , iter  161  done\n",
      "total time of one step: 2.3447489738464355 , reward= 3898.779853742813 , iter  162  done\n",
      "total time of one step: 2.663123846054077 , reward= 4055.13199271922 , iter  163  done\n",
      "total time of one step: 2.2733042240142822 , reward= 3998.3656315518833 , iter  164  done\n",
      "total time of one step: 2.596421718597412 , reward= 3929.1029454762747 , iter  165  done\n",
      "total time of one step: 2.401047945022583 , reward= 3874.7430435429596 , iter  166  done\n",
      "total time of one step: 2.3237109184265137 , reward= 4092.206034931935 , iter  167  done\n",
      "total time of one step: 2.325084924697876 , reward= 4344.939582861409 , iter  168  done\n",
      "total time of one step: 2.4293060302734375 , reward= -1506.7715152136918 , iter  169  done\n",
      "total time of one step: 2.4481430053710938 , reward= 4241.651744499695 , iter  170  done\n",
      "total time of one step: 2.2923471927642822 , reward= 3977.3736685133313 , iter  171  done\n",
      "total time of one step: 2.472853899002075 , reward= 3932.8159960579933 , iter  172  done\n",
      "total time of one step: 2.358196973800659 , reward= 3697.0116100223013 , iter  173  done\n",
      "total time of one step: 2.4894659519195557 , reward= 3860.6846154416216 , iter  174  done\n",
      "total time of one step: 2.420456886291504 , reward= 3814.8755869995834 , iter  175  done\n",
      "total time of one step: 2.3644227981567383 , reward= 3996.4036896141683 , iter  176  done\n",
      "total time of one step: 2.5904908180236816 , reward= 3956.067046065889 , iter  177  done\n",
      "total time of one step: 2.5014607906341553 , reward= 4033.2312011609342 , iter  178  done\n",
      "total time of one step: 2.5053329467773438 , reward= 3915.711933538806 , iter  179  done\n",
      "total time of one step: 2.363487958908081 , reward= 3734.373735830204 , iter  180  done\n",
      "total time of one step: 2.301719903945923 , reward= 3899.3723743164865 , iter  181  done\n",
      "total time of one step: 2.5267257690429688 , reward= 4108.42837539612 , iter  182  done\n",
      "total time of one step: 2.4211580753326416 , reward= 4073.442855315924 , iter  183  done\n",
      "total time of one step: 2.6043100357055664 , reward= 4094.8338396584236 , iter  184  done\n",
      "total time of one step: 2.530257225036621 , reward= 4089.188514095481 , iter  185  done\n",
      "total time of one step: 2.8743937015533447 , reward= 4442.00321338261 , iter  186  done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time of one step: 2.550812005996704 , reward= 4329.298807414825 , iter  187  done\n",
      "total time of one step: 2.5724809169769287 , reward= 4390.356277539324 , iter  188  done\n",
      "total time of one step: 2.3370931148529053 , reward= 4266.33289155821 , iter  189  done\n",
      "total time of one step: 2.337106943130493 , reward= 4384.828435530442 , iter  190  done\n",
      "total time of one step: 2.336374044418335 , reward= 4497.734216146323 , iter  191  done\n",
      "total time of one step: 2.360903024673462 , reward= 4353.98436518564 , iter  192  done\n",
      "total time of one step: 2.3696088790893555 , reward= 4361.791785286776 , iter  193  done\n",
      "total time of one step: 2.452301025390625 , reward= 4411.906638998138 , iter  194  done\n",
      "total time of one step: 2.3881897926330566 , reward= 4530.60263704686 , iter  195  done\n",
      "total time of one step: 2.3469419479370117 , reward= 4671.7556598454175 , iter  196  done\n",
      "total time of one step: 2.435854196548462 , reward= 4623.2005720667285 , iter  197  done\n",
      "total time of one step: 2.3244240283966064 , reward= 4592.042493703844 , iter  198  done\n",
      "total time of one step: 2.8401410579681396 , reward= 4628.772292147774 , iter  199  done\n",
      "total time of one step: 2.427873134613037 , reward= 4599.706526468884 , iter  200  done\n",
      "total time of one step: 2.319704055786133 , reward= 4638.3893452475095 , iter  201  done\n",
      "total time of one step: 2.5484042167663574 , reward= 4459.835720132869 , iter  202  done\n",
      "total time of one step: 2.5563058853149414 , reward= 4310.402986932255 , iter  203  done\n",
      "total time of one step: 2.5869507789611816 , reward= 4263.14279133754 , iter  204  done\n",
      "total time of one step: 2.5043818950653076 , reward= 4694.097330328798 , iter  205  done\n",
      "total time of one step: 2.418689012527466 , reward= 4461.425178656135 , iter  206  done\n",
      "total time of one step: 2.7933640480041504 , reward= 4417.113078005698 , iter  207  done\n",
      "total time of one step: 2.832667350769043 , reward= 4628.136826385889 , iter  208  done\n",
      "total time of one step: 2.5269181728363037 , reward= 4564.287869459224 , iter  209  done\n",
      "total time of one step: 2.271735191345215 , reward= 4636.013827491759 , iter  210  done\n",
      "total time of one step: 2.4894309043884277 , reward= 4613.699527394642 , iter  211  done\n",
      "total time of one step: 2.2246968746185303 , reward= 4645.342599402172 , iter  212  done\n",
      "total time of one step: 2.372542142868042 , reward= 4716.477966560346 , iter  213  done\n",
      "total time of one step: 2.2133049964904785 , reward= 4393.865063072464 , iter  214  done\n",
      "total time of one step: 2.3298580646514893 , reward= 4513.020657359296 , iter  215  done\n",
      "total time of one step: 2.374406099319458 , reward= 4704.983496806814 , iter  216  done\n",
      "total time of one step: 2.3502440452575684 , reward= 4692.87035966489 , iter  217  done\n",
      "total time of one step: 2.207437038421631 , reward= 4593.854145354908 , iter  218  done\n",
      "total time of one step: 2.333301067352295 , reward= 4588.12357925045 , iter  219  done\n",
      "total time of one step: 2.247450113296509 , reward= 4614.390780084457 , iter  220  done\n",
      "total time of one step: 2.3751678466796875 , reward= 4410.141809282874 , iter  221  done\n",
      "total time of one step: 2.3335530757904053 , reward= 4369.075570452067 , iter  222  done\n",
      "total time of one step: 2.386924982070923 , reward= 4627.238052580213 , iter  223  done\n",
      "total time of one step: 2.2579610347747803 , reward= 4436.085510478673 , iter  224  done\n",
      "total time of one step: 2.4703900814056396 , reward= 4352.067174091943 , iter  225  done\n",
      "total time of one step: 2.307297706604004 , reward= 4451.260412977188 , iter  226  done\n",
      "total time of one step: 2.391535997390747 , reward= 4636.011331957584 , iter  227  done\n",
      "total time of one step: 2.3817451000213623 , reward= 4631.2928399070815 , iter  228  done\n",
      "total time of one step: 2.276205062866211 , reward= 4573.534551547959 , iter  229  done\n",
      "total time of one step: 2.766608953475952 , reward= 4547.809943550605 , iter  230  done\n",
      "total time of one step: 2.2828102111816406 , reward= 4635.339899877166 , iter  231  done\n",
      "total time of one step: 2.3387598991394043 , reward= 4480.808349936683 , iter  232  done\n",
      "total time of one step: 2.4438579082489014 , reward= 4396.367378492893 , iter  233  done\n",
      "total time of one step: 2.635327100753784 , reward= 4524.489482508046 , iter  234  done\n",
      "total time of one step: 2.4439589977264404 , reward= 4501.405371293165 , iter  235  done\n",
      "total time of one step: 2.3747153282165527 , reward= 4538.952577704465 , iter  236  done\n",
      "total time of one step: 2.481827974319458 , reward= 4564.181144392064 , iter  237  done\n",
      "total time of one step: 2.263684034347534 , reward= 4571.629195057726 , iter  238  done\n",
      "total time of one step: 2.488646984100342 , reward= 4753.988258735661 , iter  239  done\n",
      "total time of one step: 2.4627039432525635 , reward= 4712.292471170801 , iter  240  done\n",
      "total time of one step: 2.3952550888061523 , reward= 4629.396296664064 , iter  241  done\n",
      "total time of one step: 2.468709945678711 , reward= 4460.195990779202 , iter  242  done\n",
      "total time of one step: 2.4876742362976074 , reward= 4561.2126776689865 , iter  243  done\n",
      "total time of one step: 2.607654094696045 , reward= 4520.8642244910525 , iter  244  done\n",
      "total time of one step: 2.464146852493286 , reward= 4740.806605869903 , iter  245  done\n",
      "total time of one step: 2.347382068634033 , reward= 4671.064704822089 , iter  246  done\n",
      "total time of one step: 2.3812098503112793 , reward= 4767.654672046367 , iter  247  done\n",
      "total time of one step: 2.289060115814209 , reward= 4688.031719437488 , iter  248  done\n",
      "total time of one step: 2.5204060077667236 , reward= 4643.090173565749 , iter  249  done\n",
      "total time of one step: 2.246755838394165 , reward= 4434.015656704115 , iter  250  done\n",
      "total time of one step: 2.314499855041504 , reward= 4527.939581317856 , iter  251  done\n",
      "total time of one step: 2.2453041076660156 , reward= 4364.603857244127 , iter  252  done\n",
      "total time of one step: 2.30954909324646 , reward= 4521.408795831932 , iter  253  done\n",
      "total time of one step: 2.3486993312835693 , reward= 4233.374292840516 , iter  254  done\n",
      "total time of one step: 2.509381055831909 , reward= 4154.273507850039 , iter  255  done\n",
      "total time of one step: 2.5836758613586426 , reward= 4291.119985661027 , iter  256  done\n",
      "total time of one step: 2.524030923843384 , reward= 4609.466100035361 , iter  257  done\n",
      "total time of one step: 2.5906081199645996 , reward= 4562.6994979596 , iter  258  done\n",
      "total time of one step: 2.318695306777954 , reward= 4382.984032267931 , iter  259  done\n",
      "total time of one step: 2.7542271614074707 , reward= 4267.676255049196 , iter  260  done\n",
      "total time of one step: 2.556549310684204 , reward= 4532.5395388528605 , iter  261  done\n",
      "total time of one step: 2.562533140182495 , reward= 4670.510497745586 , iter  262  done\n",
      "total time of one step: 2.614497184753418 , reward= 4692.848931475061 , iter  263  done\n",
      "total time of one step: 2.2405178546905518 , reward= 4573.662700246817 , iter  264  done\n",
      "total time of one step: 2.498694896697998 , reward= 4611.997576243062 , iter  265  done\n",
      "total time of one step: 2.5151472091674805 , reward= 4689.422095359641 , iter  266  done\n",
      "total time of one step: 2.3399550914764404 , reward= 4655.9530419947605 , iter  267  done\n",
      "total time of one step: 2.4021718502044678 , reward= 4634.045772996015 , iter  268  done\n",
      "total time of one step: 2.505366563796997 , reward= 4668.136751811397 , iter  269  done\n",
      "total time of one step: 2.551647901535034 , reward= 4601.365805198038 , iter  270  done\n",
      "total time of one step: 2.4516067504882812 , reward= 4548.359289791903 , iter  271  done\n",
      "total time of one step: 2.394497871398926 , reward= 4508.455248536364 , iter  272  done\n",
      "total time of one step: 2.391869068145752 , reward= 4350.362950242318 , iter  273  done\n",
      "total time of one step: 2.50173282623291 , reward= 4480.461255065697 , iter  274  done\n",
      "total time of one step: 2.7927048206329346 , reward= 4495.340879429154 , iter  275  done\n",
      "total time of one step: 2.414307117462158 , reward= 4505.595191994897 , iter  276  done\n",
      "total time of one step: 2.552051067352295 , reward= 4355.820246346905 , iter  277  done\n",
      "total time of one step: 2.719320774078369 , reward= 4552.733791803051 , iter  278  done\n",
      "total time of one step: 2.2788450717926025 , reward= 4415.499556578656 , iter  279  done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time of one step: 2.4039649963378906 , reward= 4358.490024929984 , iter  280  done\n",
      "total time of one step: 2.2913782596588135 , reward= 4552.507235647334 , iter  281  done\n",
      "total time of one step: 2.6911890506744385 , reward= 4284.442959083113 , iter  282  done\n",
      "total time of one step: 3.3714311122894287 , reward= 4597.492904695417 , iter  283  done\n",
      "total time of one step: 3.308701753616333 , reward= 4568.034205860775 , iter  284  done\n",
      "total time of one step: 3.1416633129119873 , reward= 4696.096730937096 , iter  285  done\n",
      "total time of one step: 2.5231475830078125 , reward= 4509.483533420056 , iter  286  done\n",
      "total time of one step: 2.2798659801483154 , reward= 4505.7785897359345 , iter  287  done\n",
      "total time of one step: 2.484851121902466 , reward= 4810.962744348545 , iter  288  done\n",
      "total time of one step: 2.3863701820373535 , reward= 4733.191008199599 , iter  289  done\n",
      "total time of one step: 2.53358793258667 , reward= 4920.830719881392 , iter  290  done\n",
      "total time of one step: 2.5866551399230957 , reward= 4704.7135486274 , iter  291  done\n",
      "total time of one step: 2.5125880241394043 , reward= 4668.474435354645 , iter  292  done\n",
      "total time of one step: 2.679349660873413 , reward= 4769.032969584517 , iter  293  done\n",
      "total time of one step: 2.7429330348968506 , reward= 4547.767558620444 , iter  294  done\n",
      "total time of one step: 2.508916139602661 , reward= 4762.75022529403 , iter  295  done\n",
      "total time of one step: 2.473672866821289 , reward= 4683.004783811648 , iter  296  done\n",
      "total time of one step: 2.3144638538360596 , reward= 4480.124832692936 , iter  297  done\n",
      "total time of one step: 2.506258964538574 , reward= 4705.119459002023 , iter  298  done\n",
      "total time of one step: 2.5604259967803955 , reward= 4609.630851389837 , iter  299  done\n",
      "total time of one step: 2.3569819927215576 , reward= 4691.567096683711 , iter  300  done\n",
      "total time of one step: 2.3605258464813232 , reward= 4830.775224363702 , iter  301  done\n",
      "total time of one step: 2.4783592224121094 , reward= 4591.727219690022 , iter  302  done\n",
      "total time of one step: 3.0244429111480713 , reward= 4996.60968262079 , iter  303  done\n",
      "total time of one step: 2.7945501804351807 , reward= 4888.414601332803 , iter  304  done\n",
      "total time of one step: 2.339755058288574 , reward= 4916.911809562427 , iter  305  done\n",
      "total time of one step: 3.011478900909424 , reward= 4667.726053247311 , iter  306  done\n",
      "total time of one step: 2.5462119579315186 , reward= 4940.940160673558 , iter  307  done\n",
      "total time of one step: 2.787079095840454 , reward= 5140.097947084772 , iter  308  done\n",
      "total time of one step: 2.3858439922332764 , reward= 4879.728708233659 , iter  309  done\n",
      "total time of one step: 2.724109172821045 , reward= 4950.898742962016 , iter  310  done\n",
      "total time of one step: 2.672382116317749 , reward= 4999.470767679652 , iter  311  done\n",
      "total time of one step: 2.3701322078704834 , reward= 4787.429239472437 , iter  312  done\n",
      "total time of one step: 2.676819086074829 , reward= 4680.84261998837 , iter  313  done\n",
      "total time of one step: 2.740827798843384 , reward= 4754.507046848425 , iter  314  done\n",
      "total time of one step: 2.683518886566162 , reward= 4733.327160317876 , iter  315  done\n",
      "total time of one step: 2.3623340129852295 , reward= 4654.446065236176 , iter  316  done\n",
      "total time of one step: 2.3704421520233154 , reward= 4555.808618405355 , iter  317  done\n",
      "total time of one step: 2.798419952392578 , reward= 4926.345197572384 , iter  318  done\n",
      "total time of one step: 2.727537155151367 , reward= 5051.901155538725 , iter  319  done\n",
      "total time of one step: 2.5401132106781006 , reward= 4978.923914317957 , iter  320  done\n",
      "total time of one step: 2.7758021354675293 , reward= 4680.799737095512 , iter  321  done\n",
      "total time of one step: 2.5002341270446777 , reward= 4581.617356075158 , iter  322  done\n",
      "total time of one step: 3.353350877761841 , reward= 4794.773879919832 , iter  323  done\n",
      "total time of one step: 2.564816951751709 , reward= 4517.1829938936535 , iter  324  done\n",
      "total time of one step: 2.4887571334838867 , reward= 4416.430511633849 , iter  325  done\n",
      "total time of one step: 2.9673967361450195 , reward= 4413.879391114875 , iter  326  done\n",
      "total time of one step: 3.2870030403137207 , reward= 4790.446076361021 , iter  327  done\n",
      "total time of one step: 3.0695929527282715 , reward= 4859.388382370572 , iter  328  done\n",
      "total time of one step: 2.8584158420562744 , reward= 5160.329674257225 , iter  329  done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-219963603e4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                      mode)\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mLearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-6b4ab32f0bef>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_rollouts, max_b)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mresult_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;31m#Gather the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-2bb909081065>\u001b[0m in \u001b[0;36mdo_rollout\u001b[0;34m(self, num_rollouts, normalizer, policy)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Getting the positive rewards in the positive directions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mpositive_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"positive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Getting the negative rewards in the negative/opposite directions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-2bb909081065>\u001b[0m in \u001b[0;36mexplore\u001b[0;34m(self, env, normalizer, policy, direction, delta)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0msum_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum_plays\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/envs/mujoco/half_cheetah.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mxposbefore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mxposafter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36mdo_simulation\u001b[0;34m(self, ctrl, n_frames)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctrl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Test the algorithm\n",
    "env_name = 'HalfCheetah-v2'\n",
    "env = gym.make(env_name)\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.shape[0]\n",
    "policy_params={'ob_dim':ob_dim,'ac_dim':ac_dim}\n",
    "learning_rate=0.02\n",
    "delta_std=0.03\n",
    "num_iter=1000\n",
    "num_rollouts = 8\n",
    "max_b=8\n",
    "seed = 123\n",
    "mode=1\n",
    "Learner = ARSLearner(env_name,\n",
    "                     policy_params,\n",
    "                     learning_rate,\n",
    "                     delta_std, \n",
    "                     num_iter,\n",
    "                     seed,\n",
    "                     mode)\n",
    "\n",
    "Learner.train(num_rollouts,max_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Learner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-46a936cc3a30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0misRender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Learner' is not defined"
     ]
    }
   ],
   "source": [
    "Learner.evaluate(test_num=10,isRender=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
