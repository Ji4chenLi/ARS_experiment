{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'HalfCheetah-v2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shared_noise():\n",
    "    \"\"\"\n",
    "    Create a large array of noise to be shared by all workers. Used \n",
    "    for avoiding the communication of the random perturbations delta.\n",
    "    \"\"\"\n",
    "\n",
    "    seed = 12345\n",
    "    count = 250000000\n",
    "    noise = np.random.RandomState(seed).randn(count).astype(np.float64)\n",
    "    return noise\n",
    "\n",
    "\n",
    "class SharedNoiseTable(object):\n",
    "    def __init__(self, noise, seed = 11):\n",
    "\n",
    "        self.rg = np.random.RandomState(seed)\n",
    "        self.noise = noise\n",
    "        assert self.noise.dtype == np.float64\n",
    "\n",
    "    def get(self, i, dim):\n",
    "        return self.noise[i:i + dim]\n",
    "\n",
    "    def sample_index(self, dim):\n",
    "        return self.rg.randint(0, len(self.noise) - dim + 1)\n",
    "\n",
    "    def get_delta(self, dim):\n",
    "        idx = self.sample_index(dim)\n",
    "        return idx, self.get(idx, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(object):\n",
    "    \"\"\" \n",
    "    Object class for parallel rollout generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_seed,\n",
    "                 env_name='',\n",
    "                 policy_params = None,\n",
    "                 deltas=None,\n",
    "                 rollout_length=1000,\n",
    "                 delta_std=0.02\n",
    "                 mode=1\n",
    "                 mean=None\n",
    "                 sigma=None):\n",
    "        \n",
    "        # initialize OpenAI environment for each worker\n",
    "        \n",
    "        self.env = gym.make('env_name')\n",
    "        self.env.seed(env_seed)\n",
    "        self.delta = SharedNoiseTable(deltas,seed=env_seed + 7)\n",
    "        \n",
    "        self.ob_dim = env.observation_space.shape[0]\n",
    "        self.ac_dim = env.action_space.shape[0]\n",
    "        self.policy = policy()\n",
    "        def rollout(self):\n",
    "            ob = env.reset()\n",
    "            total_reward = 0\n",
    "            for t in itertools():\n",
    "                action = self.policy.get_action(ob)\n",
    "                ob,reward,done,_ = env.step(action)\n",
    "                total_step = t\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "            return total_reward,total_step\n",
    "        def do_rollout(self, weight, num_rollouts = 1):\n",
    "            rollout_rewards,deltas_idx  = [],[]\n",
    "            steps = 0\n",
    "            \n",
    "            for rollout in range(num_rollouts):\n",
    "                \n",
    "                delta = self.delta.get_delta(weight.shape)\n",
    "                # compute reward and number of timesteps used for positive perturbation rollout\n",
    "                self.policy.update_weights(weight + delta)\n",
    "                pos_reward, pos_steps  = self.rollout(shift = shift)\n",
    "\n",
    "                # compute reward and number of timesteps used for negative pertubation rollout\n",
    "                self.policy.update_weights(w_policy - delta)\n",
    "                neg_reward, neg_steps = self.rollout(shift = shift) \n",
    "                steps += pos_steps + neg_steps\n",
    "\n",
    "                rollout_rewards.append([pos_reward, neg_reward])\n",
    "                \n",
    "            return {'deltas_idx': deltas_idx, 'rollout_rewards': rollout_rewards, \"steps\" : steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARSLearner(object):\n",
    "    \"\"\" \n",
    "    Object class implementing the ARS algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, env_name='HalfCheetah-v1',\n",
    "                 policy_params=None,\n",
    "                 max_b=4\n",
    "                 num_worker=8,\n",
    "                 delta_std=0.02, \n",
    "                 rollout_length=1000,\n",
    "                 step_size=0.01,\n",
    "                 params=None,\n",
    "                 seed=123\n",
    "                ):\n",
    "        # create shared table for storing noise\n",
    "        print(\"Creating deltas table.\")\n",
    "        deltas_id = create_shared_noise.remote()\n",
    "        self.deltas = SharedNoiseTable(ray.get(deltas_id), seed = seed + 3)\n",
    "        print('Created deltas table.')\n",
    "        \n",
    "        self.num_workers = num_workers\n",
    "        self.workers = [Worker.remote(seed + 7 * i,\n",
    "                                      env_name=env_name,\n",
    "                                      policy_params=policy_params,\n",
    "                                      deltas=deltas_id,\n",
    "                                      rollout_length=rollout_length,\n",
    "                                      delta_std=delta_std) for i in range(num_workers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy(object):\n",
    "     def __init__(self, policy_params):\n",
    "\n",
    "        self.ob_dim = policy_params['ob_dim']\n",
    "        self.ac_dim = policy_params['ac_dim']\n",
    "        self.weight = np.zeros([self.ac_dim,ob_dim])\n",
    "    def update_weights(self, new_M):\n",
    "        self.M = new_M\n",
    "        return\n",
    "    def get_action(self,ob,mode,mean):\n",
    "        if mode == 1:\n",
    "            return np.dot(self.weight,ob)\n",
    "        if mode == 2:\n",
    "            return np.dot(self.weight,(ob - mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[ 0.94879425  0.31589472 -0.16484676]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(env.action_space.shape[0])\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.]\n"
     ]
    }
   ],
   "source": [
    "#Define the hyperparameters\n",
    "\n",
    "alpha = 0.01\n",
    "v = 0.03\n",
    "N = 8\n",
    "b = 4\n",
    "\n",
    "n = 4\n",
    "p = 2\n",
    "\n",
    "M = np.zeros([p,n])\n",
    "miu = np.zeros([n])\n",
    "Sigma = np.ones([n,n])\n",
    "\n",
    "print(env.action_space.high)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
