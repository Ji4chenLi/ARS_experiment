{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shared_noise():\n",
    "    \"\"\"\n",
    "    Create a large array of noise to be shared by all workers. Used \n",
    "    for avoiding the communication of the random perturbations delta.\n",
    "    \"\"\"\n",
    "\n",
    "    seed = 12345\n",
    "    count = 250000000\n",
    "    noise = np.random.RandomState(seed).randn(count).astype(np.float64)\n",
    "    return noise\n",
    "\n",
    "\n",
    "class SharedNoiseTable(object):\n",
    "    def __init__(self, noise, seed = 11):\n",
    "\n",
    "        self.rg = np.random.RandomState(seed)\n",
    "        self.noise = noise\n",
    "        assert self.noise.dtype == np.float64\n",
    "\n",
    "    def get(self, i, dim):\n",
    "        return self.noise[i:i + dim]\n",
    "\n",
    "    def sample_index(self, dim):\n",
    "        return self.rg.randint(0, len(self.noise) - dim + 1)\n",
    "\n",
    "    def get_delta(self, dim):\n",
    "        idx = self.sample_index(dim)\n",
    "        return idx, self.get(idx, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(object):\n",
    "    \"\"\" \n",
    "    Object class for parallel rollout generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_seed,\n",
    "                 env_name='',\n",
    "                 policy_params=None,\n",
    "                 deltas=None,\n",
    "                 delta_std=0.02,\n",
    "                 mode=1\n",
    "                 ):\n",
    "        \n",
    "        # initialize OpenAI environment for each worker\n",
    "        \n",
    "        self.env = gym.make(env_name)\n",
    "        self.env.seed(env_seed)\n",
    "        self.deltas = SharedNoiseTable(deltas,seed=env_seed + 7)\n",
    "        \n",
    "        self.ob_dim = env.observation_space.shape[0]\n",
    "        self.ac_dim = env.action_space.shape[0]\n",
    "        print(policy_params)\n",
    "        self.policy = Policy(policy_params)\n",
    "        self.delta_std = delta_std\n",
    "        \n",
    "    def rollout(self,mode,mean,Sigma):\n",
    "        ob = env.reset()\n",
    "        total_ob = [ob]\n",
    "        total_reward = 0\n",
    "        for t in itertools.count():\n",
    "            action = self.policy.get_action(ob,mode,mean,Sigma)\n",
    "            ob,reward,done,_ = env.step(action)\n",
    "            total_ob.append(ob)\n",
    "            total_step = t\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return total_reward,total_step,total_ob\n",
    "\n",
    "    def do_rollout(self, weight, num_rollouts,mode,mean,Sigma):\n",
    "        rollout_rewards,deltas_idx,rollout_ob  = [],[],[]\n",
    "        steps = 0\n",
    "\n",
    "        for i_rollout in range(num_rollouts):\n",
    "\n",
    "            idx, delta = self.deltas.get_delta(weight.size)\n",
    "\n",
    "            delta = (self.delta_std * delta).reshape(weight.shape)\n",
    "            deltas_idx.append(idx)\n",
    "            # compute reward and number of timesteps used for positive perturbation rollout\n",
    "            self.policy.update_weights(weight + self.delta_std * delta)\n",
    "            pos_reward, pos_steps, pos_ob  = self.rollout(mode,mean,Sigma)\n",
    "\n",
    "            # compute reward and number of timesteps used for negative pertubation rollout\n",
    "            self.policy.update_weights(weight - self.delta_std * delta)\n",
    "            neg_reward, neg_steps, neg_ob = self.rollout(mode,mean,Sigma) \n",
    "            steps += pos_steps + neg_steps\n",
    "\n",
    "            rollout_rewards.append([pos_reward, neg_reward])\n",
    "            rollout_ob.append([pos_ob,neg_ob])\n",
    "\n",
    "        return {'deltas_idx': deltas_idx, 'rollout_rewards': rollout_rewards,\n",
    "                \"steps\" : steps, \"rollout_ob\": rollout_ob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARSLearner(object):\n",
    "    \"\"\" \n",
    "    Object class implementing the ARS algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, env_name='HalfCheetah-v2',\n",
    "                 policy_params=None,\n",
    "                 l_rate=0.02,\n",
    "                 num_workers=1,\n",
    "                 delta_std=0.02, \n",
    "                 num_iter=1000,\n",
    "                 step_size=0.01,\n",
    "                 seed=123,\n",
    "                 mode=1\n",
    "                ):\n",
    "        # create shared table for storing noise\n",
    "        print(\"Creating deltas table.\")\n",
    "        deltas_id = create_shared_noise()\n",
    "        self.deltas = SharedNoiseTable(deltas_id, seed = seed + 3)\n",
    "        self.l_rate = l_rate\n",
    "        self.workers = [Worker(seed + 7 * i,\n",
    "                               env_name=env_name,\n",
    "                               policy_params=policy_params,\n",
    "                               deltas=deltas_id,\n",
    "                               delta_std=delta_std,\n",
    "                               mode=1) for i in range(num_workers)]\n",
    "    def train(self,num_iter,num_rollouts,max_b,mode,policy_params):\n",
    "        \n",
    "        weight = np.zeros([policy_params['ob_dim'],policy_params['ac_dim']])\n",
    "        mean = 0\n",
    "        Sigma = np.identity(policy_params['ob_dim'])\n",
    "\n",
    "        for t in range(num_iter):\n",
    "            t1 = time.time()\n",
    "            result_list = [worker.do_rollout(weight,num_rollouts,mode,mean,Sigma) for worker in self.workers]\n",
    "            t2 = time.time()\n",
    "            print('total time of one step', t2 - t1)           \n",
    "            print('iter ', t,' done')\n",
    "            \n",
    "            #Gather the result\n",
    "\n",
    "            ob_list = [iob for result in result_list for ob in result['rollout_ob'] for iob in ob]\n",
    "            ob_array = np.array(ob_list)\n",
    "            mean = np.mean(ob_array)\n",
    "            Sigma = np.std(ob_array)\n",
    "            \n",
    "            result_dict = result_list[0]\n",
    "            roll_reward = result_dict['rollout_rewards']\n",
    "            roll_delidx = result_dict['deltas_idx']\n",
    "            r_list = zip(roll_reward,roll_delidx)\n",
    "\n",
    "            r_list = sorted(r_list,key=lambda tup: max(tup[0][0],tup[0][1]))\n",
    "            r_list = r_list[0:max_b]\n",
    "            #Update the weight\n",
    "            reward_list = []\n",
    "            for roll_r,delta_id in r_list:\n",
    "                cum_diff = (roll_r[0] - roll_r[1]) * self.deltas.get(delta_id,weight.size)\n",
    "                reward_list.append(roll_r[0])\n",
    "                reward_list.append(roll_r[1])\n",
    "            reward_list = np.array(reward_list)\n",
    "            weight = weight + self.l_rate / np.std(reward_list) * cum_diff.reshape(weight.shape)\n",
    "            \n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "    def __init__(self, policy_params):\n",
    "\n",
    "        self.ob_dim = policy_params['ob_dim']\n",
    "        self.ac_dim = policy_params['ac_dim']\n",
    "        self.weight = np.zeros([self.ac_dim,ob_dim])\n",
    "    def update_weights(self, new_weight):\n",
    "        self.weight = new_weight\n",
    "        return\n",
    "    def get_action(self,ob,mode,mean,Sigma):\n",
    "        if mode == 1:\n",
    "            return np.matmul(self.weight,ob)\n",
    "        if mode == 2:\n",
    "            Sigma_diag = np.diag(np.diag(Sigma))\n",
    "            Sigma_t = np.sqrt(np.linalg.inv(Sigma_diag))\n",
    "            return np.matmul(np.matmul(self.weight,Sigma_t),(ob - mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating deltas table.\n",
      "{'ob_dim': 6, 'ac_dim': 17}\n",
      "total time of one step 2.458125114440918\n",
      "iter  0  done\n",
      "total time of one step 2.2124271392822266\n",
      "iter  1  done\n",
      "total time of one step 1.886685848236084\n",
      "iter  2  done\n",
      "total time of one step 1.8678677082061768\n",
      "iter  3  done\n",
      "total time of one step 2.3542778491973877\n",
      "iter  4  done\n",
      "total time of one step 1.6897187232971191\n",
      "iter  5  done\n",
      "total time of one step 1.9016399383544922\n",
      "iter  6  done\n",
      "total time of one step 1.6514859199523926\n",
      "iter  7  done\n",
      "total time of one step 2.273731231689453\n",
      "iter  8  done\n",
      "total time of one step 2.559812068939209\n",
      "iter  9  done\n",
      "total time of one step 2.9243950843811035\n",
      "iter  10  done\n",
      "total time of one step 1.7537341117858887\n",
      "iter  11  done\n",
      "total time of one step 3.411369800567627\n",
      "iter  12  done\n",
      "total time of one step 1.8797550201416016\n",
      "iter  13  done\n",
      "total time of one step 2.0629491806030273\n",
      "iter  14  done\n",
      "total time of one step 1.8385827541351318\n",
      "iter  15  done\n",
      "total time of one step 2.700763702392578\n",
      "iter  16  done\n",
      "total time of one step 2.285688638687134\n",
      "iter  17  done\n",
      "total time of one step 2.808448076248169\n",
      "iter  18  done\n",
      "total time of one step 1.8554139137268066\n",
      "iter  19  done\n",
      "total time of one step 2.3991358280181885\n",
      "iter  20  done\n",
      "total time of one step 1.9454689025878906\n",
      "iter  21  done\n",
      "total time of one step 2.19378399848938\n",
      "iter  22  done\n",
      "total time of one step 2.2708799839019775\n",
      "iter  23  done\n",
      "total time of one step 2.2429862022399902\n",
      "iter  24  done\n",
      "total time of one step 2.4829320907592773\n",
      "iter  25  done\n",
      "total time of one step 4.071167945861816\n",
      "iter  26  done\n",
      "total time of one step 2.1867949962615967\n",
      "iter  27  done\n",
      "total time of one step 1.7326240539550781\n",
      "iter  28  done\n",
      "total time of one step 2.156167984008789\n",
      "iter  29  done\n",
      "total time of one step 2.500546932220459\n",
      "iter  30  done\n",
      "total time of one step 2.241903066635132\n",
      "iter  31  done\n",
      "total time of one step 2.398074150085449\n",
      "iter  32  done\n",
      "total time of one step 2.1371910572052\n",
      "iter  33  done\n",
      "total time of one step 2.0921988487243652\n",
      "iter  34  done\n",
      "total time of one step 1.8879399299621582\n",
      "iter  35  done\n",
      "total time of one step 1.7181930541992188\n",
      "iter  36  done\n",
      "total time of one step 2.658799171447754\n",
      "iter  37  done\n",
      "total time of one step 1.8902587890625\n",
      "iter  38  done\n",
      "total time of one step 2.7117598056793213\n",
      "iter  39  done\n",
      "total time of one step 2.234890937805176\n",
      "iter  40  done\n",
      "total time of one step 2.6048309803009033\n",
      "iter  41  done\n",
      "total time of one step 1.7049429416656494\n",
      "iter  42  done\n",
      "total time of one step 2.4253969192504883\n",
      "iter  43  done\n",
      "total time of one step 1.9778780937194824\n",
      "iter  44  done\n",
      "total time of one step 2.018829822540283\n",
      "iter  45  done\n",
      "total time of one step 2.067870855331421\n",
      "iter  46  done\n",
      "total time of one step 1.9292149543762207\n",
      "iter  47  done\n",
      "total time of one step 2.135195255279541\n",
      "iter  48  done\n",
      "total time of one step 2.0983707904815674\n",
      "iter  49  done\n",
      "total time of one step 1.6604440212249756\n",
      "iter  50  done\n",
      "total time of one step 1.9187910556793213\n",
      "iter  51  done\n",
      "total time of one step 2.1285669803619385\n",
      "iter  52  done\n",
      "total time of one step 1.806102991104126\n",
      "iter  53  done\n",
      "total time of one step 2.458195924758911\n",
      "iter  54  done\n",
      "total time of one step 2.280421257019043\n",
      "iter  55  done\n",
      "total time of one step 2.0452888011932373\n",
      "iter  56  done\n",
      "total time of one step 2.0242371559143066\n",
      "iter  57  done\n",
      "total time of one step 2.5914361476898193\n",
      "iter  58  done\n",
      "total time of one step 1.9020390510559082\n",
      "iter  59  done\n",
      "total time of one step 1.9809489250183105\n",
      "iter  60  done\n",
      "total time of one step 1.8069262504577637\n",
      "iter  61  done\n",
      "total time of one step 1.695261001586914\n",
      "iter  62  done\n",
      "total time of one step 1.80802583694458\n",
      "iter  63  done\n",
      "total time of one step 1.7536978721618652\n",
      "iter  64  done\n",
      "total time of one step 2.8203139305114746\n",
      "iter  65  done\n",
      "total time of one step 2.0349082946777344\n",
      "iter  66  done\n",
      "total time of one step 2.662000894546509\n",
      "iter  67  done\n",
      "total time of one step 1.812608003616333\n",
      "iter  68  done\n",
      "total time of one step 2.1691501140594482\n",
      "iter  69  done\n",
      "total time of one step 1.837862253189087\n",
      "iter  70  done\n",
      "total time of one step 2.2062458992004395\n",
      "iter  71  done\n",
      "total time of one step 2.10568904876709\n",
      "iter  72  done\n",
      "total time of one step 2.04091215133667\n",
      "iter  73  done\n",
      "total time of one step 1.7602179050445557\n",
      "iter  74  done\n",
      "total time of one step 2.4962611198425293\n",
      "iter  75  done\n",
      "total time of one step 1.7792010307312012\n",
      "iter  76  done\n",
      "total time of one step 1.7264552116394043\n",
      "iter  77  done\n",
      "total time of one step 1.8797590732574463\n",
      "iter  78  done\n",
      "total time of one step 1.9753530025482178\n",
      "iter  79  done\n",
      "total time of one step 2.019623041152954\n",
      "iter  80  done\n",
      "total time of one step 2.18129301071167\n",
      "iter  81  done\n",
      "total time of one step 2.0982749462127686\n",
      "iter  82  done\n",
      "total time of one step 2.5307998657226562\n",
      "iter  83  done\n",
      "total time of one step 1.6865379810333252\n",
      "iter  84  done\n",
      "total time of one step 2.2085089683532715\n",
      "iter  85  done\n",
      "total time of one step 2.408482074737549\n",
      "iter  86  done\n",
      "total time of one step 1.9904100894927979\n",
      "iter  87  done\n",
      "total time of one step 2.0607101917266846\n",
      "iter  88  done\n",
      "total time of one step 2.3894729614257812\n",
      "iter  89  done\n",
      "total time of one step 1.7644569873809814\n",
      "iter  90  done\n",
      "total time of one step 1.8312437534332275\n",
      "iter  91  done\n",
      "total time of one step 1.769223928451538\n",
      "iter  92  done\n",
      "total time of one step 4.3462440967559814\n",
      "iter  93  done\n",
      "total time of one step 1.7149531841278076\n",
      "iter  94  done\n",
      "total time of one step 2.2323079109191895\n",
      "iter  95  done\n",
      "total time of one step 1.763071060180664\n",
      "iter  96  done\n",
      "total time of one step 4.725793123245239\n",
      "iter  97  done\n",
      "total time of one step 1.6892058849334717\n",
      "iter  98  done\n",
      "total time of one step 1.9249861240386963\n",
      "iter  99  done\n",
      "total time of one step 2.2220380306243896\n",
      "iter  100  done\n",
      "total time of one step 2.138432025909424\n",
      "iter  101  done\n",
      "total time of one step 1.746495008468628\n",
      "iter  102  done\n",
      "total time of one step 2.0959270000457764\n",
      "iter  103  done\n",
      "total time of one step 1.9727559089660645\n",
      "iter  104  done\n",
      "total time of one step 2.1707029342651367\n",
      "iter  105  done\n",
      "total time of one step 1.6855261325836182\n",
      "iter  106  done\n"
     ]
    }
   ],
   "source": [
    "#Test the algorithm\n",
    "env_name = 'HalfCheetah-v2'\n",
    "env = gym.make(env_name)\n",
    "ac_dim = env.observation_space.shape[0]\n",
    "ob_dim = env.action_space.shape[0]\n",
    "policy_params={'ob_dim':ob_dim,'ac_dim':ac_dim}\n",
    "l_rate=0.02\n",
    "num_workers=1\n",
    "delta_std=0.02\n",
    "num_iter=1000\n",
    "step_size=0.01\n",
    "seed=123\n",
    "mode=1\n",
    "num_rollouts = 8\n",
    "max_b=4\n",
    "Learner = ARSLearner(env_name,\n",
    "                     policy_params,\n",
    "                     l_rate,\n",
    "                     num_workers,\n",
    "                     delta_std,\n",
    "                     num_iter,\n",
    "                     step_size,\n",
    "                     seed,\n",
    "                     mode)\n",
    "\n",
    "weight = Learner.train(num_iter,num_rollouts,max_b,mode,policy_params,)\n",
    "policy = Policy(policy_params)\n",
    "policy.update_weights(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-a83550a93dfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "env_name = 'HalfCheetah-v2'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "ob = env.reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
