{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the states\n",
    "\n",
    "class Normalizer():\n",
    "    \n",
    "    def __init__(self, nb_inputs):\n",
    "        self.n = np.zeros(nb_inputs)\n",
    "        self.mean = np.zeros(nb_inputs)\n",
    "        self.mean_diff = np.zeros(nb_inputs)\n",
    "        self.var = np.zeros(nb_inputs)\n",
    "    \n",
    "    def observe(self, x):\n",
    "        self.n += 1.\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
    "    \n",
    "    def normalize(self, inputs):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs - obs_mean) / obs_std\n",
    "        #return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(object):\n",
    "    \"\"\" \n",
    "    Object class for parallel rollout generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 env_name='Swimmer-v2',\n",
    "                 policy_params=None,\n",
    "                 delta_std=0.02,\n",
    "                 ):\n",
    "        \n",
    "        # initialize OpenAI environment for each worker\n",
    "        \n",
    "        self.env = gym.make(env_name)\n",
    "        self.delta_std = delta_std\n",
    "        \n",
    "    def explore(self, env, normalizer, policy, direction = None, delta = None):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        num_plays = 0.\n",
    "        sum_rewards = 0\n",
    "        while not done and num_plays < 1000:\n",
    "            normalizer.observe(state)\n",
    "            state = normalizer.normalize(state)\n",
    "            action = policy.evaluate(state, delta, self.delta_std, direction)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            #reward = max(min(reward, 1), -1)\n",
    "            sum_rewards += reward\n",
    "            num_plays += 1\n",
    "        return sum_rewards\n",
    "\n",
    "    def do_rollout(self, num_rollouts, normalizer, policy):    \n",
    "        # Initializing the perturbations deltas and the positive/negative rewards\n",
    "        deltas = policy.sample_deltas(num_rollouts)\n",
    "        positive_rewards = [0] * num_rollouts\n",
    "        negative_rewards = [0] * num_rollouts\n",
    "\n",
    "        # Getting the positive rewards in the positive directions\n",
    "        for k in range(num_rollouts):\n",
    "            positive_rewards[k] = self.explore(env, normalizer, policy, direction = \"positive\", delta = deltas[k])\n",
    "        # Getting the negative rewards in the negative/opposite directions\n",
    "        for k in range(num_rollouts):\n",
    "            negative_rewards[k] = self.explore(env, normalizer, policy, direction = \"negative\", delta = deltas[k])\n",
    "\n",
    "        return {'deltas': deltas, 'positive_rewards': positive_rewards, 'negative_rewards': negative_rewards}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARSLearner(object):\n",
    "    \"\"\" \n",
    "    Object class implementing the ARS algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 env_name='Swimmer-v2',\n",
    "                 policy_params=None,\n",
    "                 learning_rate=0.02,\n",
    "                 delta_std=0.02, \n",
    "                 num_iter=1000,\n",
    "                ):\n",
    "        self.policy = Policy(policy_params['ob_dim'], policy_params['ac_dim'])\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iter = num_iter\n",
    "        self.normalizer = Normalizer(policy_params['ob_dim'])          \n",
    "        self.worker = Worker(env_name=env_name,\n",
    "                             policy_params=policy_params,\n",
    "                             delta_std=delta_std)\n",
    "        \n",
    "    def train(self,num_rollouts,max_b):\n",
    "\n",
    "        for t in range(self.num_iter):\n",
    "            t1 = time.time() \n",
    "            result_dict = self.worker.do_rollout(num_rollouts, self.normalizer, self.policy) \n",
    "            #Gather the result\n",
    "\n",
    "            deltas = result_dict['deltas']\n",
    "            positive_rewards = result_dict['positive_rewards']\n",
    "            negative_rewards = result_dict['negative_rewards']\n",
    "\n",
    "            #Update the weight\n",
    "            all_rewards = np.array(positive_rewards + negative_rewards)\n",
    "            sigma_r = all_rewards.std()\n",
    "\n",
    "            # Sorting the rollouts by the max(r_pos, r_neg) and selecting the best directions\n",
    "            scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "            order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:max_b]\n",
    "            rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
    "\n",
    "            # Updating our policy\n",
    "            self.policy.update(rollouts, sigma_r, self.learning_rate, max_b)\n",
    "\n",
    "            # Printing the final reward of the policy after the update\n",
    "            reward_evaluation = self.worker.explore(env, self.normalizer, self.policy)\n",
    "            t2 = time.time() \n",
    "            print('total time of one step:', t2 - t1,', reward=' ,reward_evaluation,', iter ', t,' done')      \n",
    "    \n",
    "    def evaluate(self,test_num,isRender):\n",
    "        returns = []\n",
    "        print(self.policy.theta)\n",
    "        for i in range(test_num):\n",
    "            print('iter', i)\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            num_plays = 0\n",
    "            sum_rewards = 0\n",
    "            while not done and num_plays < 1000:\n",
    "                state = self.normalizer.normalize(state)\n",
    "                action = self.policy.evaluate(state, None, delta_std, None)\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                sum_rewards += reward\n",
    "                num_plays += 1\n",
    "                if isRender:\n",
    "                    env.render()   \n",
    "                if num_plays % 100 == 0: print(\"%i/%i\"%(steps, env.spec.timestep_limit))\n",
    "\n",
    "            returns.append(totalr)\n",
    "\n",
    "        print('returns', returns)\n",
    "        print('mean return', np.mean(returns))\n",
    "        print('std of return', np.std(returns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Building the AI\n",
    "\n",
    "class Policy():\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.theta = np.zeros((output_size, input_size))\n",
    "    \n",
    "    def evaluate(self, input, delta = None, delta_std =  0.02, direction = None):\n",
    "        if direction is None:\n",
    "            return self.theta.dot(input)\n",
    "        elif direction == \"positive\":\n",
    "            return (self.theta + delta_std*delta).dot(input)\n",
    "        else:\n",
    "            return (self.theta - delta_std*delta).dot(input)\n",
    "    \n",
    "    def sample_deltas(self,num_rollouts):\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(num_rollouts)]\n",
    "    \n",
    "    def update(self, rollouts, sigma_r,learning_rate,max_b):\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for r_pos, r_neg, d in rollouts:\n",
    "            step += (r_pos - r_neg) * d\n",
    "        self.theta += learning_rate / (max_b * sigma_r) * step\n",
    "\n",
    "# Exploring the policy on one specific direction and over one episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time of one step: 3.100667953491211 , reward= -0.09666300303047773 , iter  0  done\n",
      "total time of one step: 4.585129022598267 , reward= -0.9899459044483576 , iter  1  done\n",
      "total time of one step: 3.810541868209839 , reward= 0.6797305277106737 , iter  2  done\n",
      "total time of one step: 3.845560073852539 , reward= 1.2707291036271393 , iter  3  done\n",
      "total time of one step: 3.269714832305908 , reward= 1.5116240143419049 , iter  4  done\n",
      "total time of one step: 2.9285519123077393 , reward= 2.928415072028285 , iter  5  done\n",
      "total time of one step: 2.846930980682373 , reward= -0.10433289236716452 , iter  6  done\n",
      "total time of one step: 3.0134689807891846 , reward= 0.23326727681880127 , iter  7  done\n",
      "total time of one step: 2.994875907897949 , reward= 373.4531294825746 , iter  8  done\n",
      "total time of one step: 2.650787115097046 , reward= 173.3880407571362 , iter  9  done\n",
      "total time of one step: 2.489946126937866 , reward= 14.613119678304253 , iter  10  done\n",
      "total time of one step: 2.48705792427063 , reward= 202.61680509608752 , iter  11  done\n",
      "total time of one step: 2.476313829421997 , reward= 601.5011030973095 , iter  12  done\n",
      "total time of one step: 2.449240207672119 , reward= 204.14388437770612 , iter  13  done\n",
      "total time of one step: 2.467144727706909 , reward= 255.95060758781185 , iter  14  done\n",
      "total time of one step: 2.4128339290618896 , reward= 350.9874752894453 , iter  15  done\n",
      "total time of one step: 2.4615659713745117 , reward= 512.6582479191123 , iter  16  done\n",
      "total time of one step: 2.5595240592956543 , reward= 595.4226156750907 , iter  17  done\n",
      "total time of one step: 2.351303815841675 , reward= 937.1463627494293 , iter  18  done\n",
      "total time of one step: 2.440897226333618 , reward= 1111.4691130180254 , iter  19  done\n",
      "total time of one step: 2.4479639530181885 , reward= 1137.7808784944536 , iter  20  done\n",
      "total time of one step: 2.344815969467163 , reward= 1002.3118207654915 , iter  21  done\n",
      "total time of one step: 2.5773520469665527 , reward= 957.0632474799655 , iter  22  done\n",
      "total time of one step: 2.388573408126831 , reward= 852.973876900962 , iter  23  done\n",
      "total time of one step: 2.4549150466918945 , reward= 908.2553498625266 , iter  24  done\n",
      "total time of one step: 2.4597103595733643 , reward= 973.464693341121 , iter  25  done\n",
      "total time of one step: 2.3740768432617188 , reward= 1066.888006321947 , iter  26  done\n",
      "total time of one step: 2.48909592628479 , reward= 769.2492790508803 , iter  27  done\n",
      "total time of one step: 2.420393228530884 , reward= 1099.2047383677532 , iter  28  done\n",
      "total time of one step: 2.432568073272705 , reward= 1198.801373602659 , iter  29  done\n",
      "total time of one step: 2.483347177505493 , reward= 1312.859289981106 , iter  30  done\n",
      "total time of one step: 2.328015089035034 , reward= 1361.636612706702 , iter  31  done\n",
      "total time of one step: 2.456744909286499 , reward= 1335.7346917338527 , iter  32  done\n",
      "total time of one step: 2.3969380855560303 , reward= 1281.8354432592555 , iter  33  done\n",
      "total time of one step: 2.3482789993286133 , reward= 1205.912016690331 , iter  34  done\n",
      "total time of one step: 2.454141139984131 , reward= 1225.6218417779517 , iter  35  done\n",
      "total time of one step: 2.4153008460998535 , reward= 1444.4971452875811 , iter  36  done\n",
      "total time of one step: 2.5724990367889404 , reward= 1415.2832686050153 , iter  37  done\n",
      "total time of one step: 2.3778698444366455 , reward= 1590.3968057301415 , iter  38  done\n",
      "total time of one step: 2.3703010082244873 , reward= 1480.3305639670975 , iter  39  done\n",
      "total time of one step: 2.500157117843628 , reward= 1430.103912173191 , iter  40  done\n",
      "total time of one step: 2.3769710063934326 , reward= 1446.3870036499727 , iter  41  done\n",
      "total time of one step: 2.4083425998687744 , reward= 1526.065677556816 , iter  42  done\n",
      "total time of one step: 2.394000768661499 , reward= 1431.8176788832989 , iter  43  done\n",
      "total time of one step: 2.361664056777954 , reward= 1475.5645048553104 , iter  44  done\n",
      "total time of one step: 2.538262128829956 , reward= 1512.6235605950203 , iter  45  done\n",
      "total time of one step: 3.28486704826355 , reward= 1489.4454857696687 , iter  46  done\n",
      "total time of one step: 3.1800551414489746 , reward= 1580.7765031272816 , iter  47  done\n",
      "total time of one step: 3.090235710144043 , reward= 1599.3118171827143 , iter  48  done\n",
      "total time of one step: 3.1339337825775146 , reward= 1662.2919795896498 , iter  49  done\n",
      "total time of one step: 3.1286709308624268 , reward= 1629.560560581496 , iter  50  done\n",
      "total time of one step: 2.5691609382629395 , reward= 1651.653952051803 , iter  51  done\n",
      "total time of one step: 2.581273078918457 , reward= 1640.5574763802902 , iter  52  done\n",
      "total time of one step: 2.3987221717834473 , reward= 1626.6378570705567 , iter  53  done\n",
      "total time of one step: 2.4632551670074463 , reward= 1710.2455719989498 , iter  54  done\n",
      "total time of one step: 2.6841540336608887 , reward= 1677.1813666831063 , iter  55  done\n",
      "total time of one step: 2.696981906890869 , reward= 1686.6849858000917 , iter  56  done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-458fa343dac6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                      num_iter)\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mLearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-124f9ddd1179>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_rollouts, max_b)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mresult_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;31m#Gather the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5826d3f4397f>\u001b[0m in \u001b[0;36mdo_rollout\u001b[0;34m(self, num_rollouts, normalizer, policy)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Getting the positive rewards in the positive directions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mpositive_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"positive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Getting the negative rewards in the negative/opposite directions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5826d3f4397f>\u001b[0m in \u001b[0;36mexplore\u001b[0;34m(self, env, normalizer, policy, direction, delta)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;31m#reward = max(min(reward, 1), -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0msum_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/envs/mujoco/half_cheetah.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mxposafter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mreward_ctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mreward_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxposafter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mxposbefore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/envs/mujoco/half_cheetah.py\u001b[0m in \u001b[0;36m_get_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m         return np.concatenate([\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqvel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         ])\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Test the algorithm\n",
    "env_name = 'HalfCheetah-v2'\n",
    "env = gym.make(env_name)\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.shape[0]\n",
    "policy_params={'ob_dim':ob_dim,'ac_dim':ac_dim}\n",
    "learning_rate=0.02\n",
    "delta_std=0.03\n",
    "num_iter=1000\n",
    "num_rollouts = 8\n",
    "max_b=8\n",
    "Learner = ARSLearner(env_name,\n",
    "                     policy_params,\n",
    "                     learning_rate,\n",
    "                     delta_std, \n",
    "                     num_iter)\n",
    "\n",
    "Learner.train(num_rollouts,max_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Learner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-46a936cc3a30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0misRender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Learner' is not defined"
     ]
    }
   ],
   "source": [
    "Learner.evaluate(test_num=10,isRender=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
